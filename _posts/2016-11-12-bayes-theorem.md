---
layout: blog
title: ナイーブベイズをキャッチアップするためにベイズの定理を復習する
category: blog
tags: [機械学習,machine learning,ベイズの定理,naive bayes]  
summary: 最近テキストを解析してごにょごにょする仕事をよくしているのですが、精度は高くないけど、
author: aharada
---

最近テキストを解析してごにょごにょする仕事をよくしているのですが、精度は高くないけど、比較的少ないデータ量でもいい感じに振る舞ってくれるナイーブベイズ分類器で実装を試したりしています。

scikit-learnを使っていると特に何も考えずに実装ができてしまうのですが、基本に立ち返ってナイーブベイズ分類器がどんなものなのかをキャッチアップしたいと思ってます。まずはベイズの定理から整理してみます。

こちらのエントリが非常に丁寧に解説されています。

[http://qiita.com/aflc/items/13fe52243c35d3b678b0](http://qiita.com/aflc/items/13fe52243c35d3b678b0)

## ベイズの定理

ひとことで言ってしまうと、ベイズの定理を使って分類問題を解こうというもので、こちらがベイズの定理です。

$$P(A_i\mid B_j)=\frac{P(B_j\mid A_i)P(A_i)}{P(B_j)}$$


さて、これでどうやって分類問題を解こうというのでしょうかね。定理を構成する項を詳しく見てみますか。

迷惑メールの例で考えて例えば以下のように表してみます。

$$A_1$$: 迷惑メールである
$$A_2$$: 迷惑メールではない
$$B_1$$: メール本文に「完全無料」という文字列を含む
$$B_2$$: メール本文に「完全無料」という文字列を含まない

## 事前確率 $$P(A_i), P(B_j)$$

既にわかっている、迷惑メールである確率、もしくはそうではない確率が入ります。例えば、日本でやりとりされているメールの6割が迷惑メールであるという統計が存在するとしましょう。

すると $$P(A_1) = 0.6$$、$$P(A_2) = 0.4$$ となります。

$$P(B_j)$$ も同様ですね。統計に使用されている全てのメールの中から「完全無料」という文字列が含まれる確率、そうではない確率が入ります。


## 事後確率 $$P(A_i\mid B_j)$$

左辺の式ですが、これが求めたい確率になるので、

$$P(A_1\mid B_1)$$: 「完全無料」という文字列が含まれるメールが迷惑メールである確率

ということになります。

## 条件付き確率 $$P(B_j\mid A_i)$$

$$P(B_1\mid A_1)$$: 迷惑メールの中に「完全無料」という文字列が含まれる確率、例えば1%とします。
$$P(B_1\mid A_2)$$: 迷惑メールではないメールの中に「完全無料」という文字列が含まれる確率、例えば、0.001%とします。

## 実際に計算してみる

では実際に計算してみます。まずはベイズの定理の右辺の分母に全確率の公式をあてはめてこのようにする(ごめんなさい時間がないので全確率の公式の詳細は割愛…)。

$$P(A_i\mid B_j)=\frac{P(B_j\mid A_i)P(A_i)}{\sum^a_{i=1}P(B_j\mid A_i)P(A_i)}$$

この式に上記で例示した事前確率を代入するとこのようになります。

$$P(A_1\mid B_1)=\frac{0.01\times 0.6}{(0.01\times 0.6)+(0.00001\times 0.4)}=0.99933377748$$

「完全無料」という文字列が含まれるメールは99.9%迷惑メールであるという結果になってしまった。。あれれ。こんな数値になるはずじゃなかった。どこか間違えてますね。あとで見直してみます。
