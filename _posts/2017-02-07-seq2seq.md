---
layout: blog
title: TensorFlowのseq2seqで対話を学習させる
category: blog
tags: [機械学習,machine learning,TensorFlow,seq2seq,RNN]
summary: 世の中ディープラーニングということで、対話システムに応用できる？とseq2seqをTensorFlowで実装してみます。
author: aharada
---

世の中ディープラーニングということで、対話システムに応用できる？とseq2seqをTensorFlowで実装してみます。英仏翻訳のチュートリアルがありますが、今回は日本語の対話でやりたかったので、下記を参考にとりあえずそのまま動かしてみることにします。

TensorFlowのseq2seqを自前のデータセットで試す

[http://qiita.com/knok/items/b24deb8cff3df48920ee](http://qiita.com/knok/items/b24deb8cff3df48920ee)

## 準備編

まずは日本語での対話データが必要です。これあんまりその辺にあるわけでもないし、ディープラーニングに流せる程度の量を自分一人で作るのは現実的ではないので、上のQiitaの記事のやり方で取得しておきます。

```
$ git clone https://github.com/knok/make-meidai-dialogue
$ cd make-meidai-dialogue
$ make all
$ nkf -w -X -Z0 sequence.txt | \
sed -ne '/^input/s/^input: //p' | mecab -Owakati > input.txt
$ nkf -w -X -Z0 sequence.txt | \
sed -ne '/^output/s/^output: //p' | mecab -Owakati > output.txt
```

input.txtはこんな感じ

```
いやー ん 、 1 人 で や だ ー 。
はい 、 ありがとう 。
岐阜 の 信用金庫 の ところ で ?
な 、 なんか ざ 、 あの 、 て 、 雑草 の 話 な ん やろ 、 あれ 。
やっぱ 東京 の 方 が 多い よ ねー って 。
略
```

output.txt

```
F 050 さん は 、 今日 何 時間 あっ た の ?
かわいい よ 、 プリクラ * 張っ とっ て も *。
お願い しま ー す 。
うん うん 。
ピクル 、 ピクルス でしょ 。
略
```

行ごとにinput/outputの対話になっているはずだと思うんだけど、ぱっと見そうは見えないがまあいいか。

これで形態素解析済みのinputとoutputの日本語文章のデータが出来ました。こういうの用意していただけるのは大変ありがたい。これがないとチュートリアルさえ出来ないのだ。

ちなみにこの対話文章データはこちらが取得元。100時間分の雑談対話を研究用に置いておいてくれてるみたい。ありがたし。

日本語自然会話書き起こしコーパス（旧名大会話コーパス）

[https://nknet.ninjal.ac.jp/nuc/templates/nuc.html](https://nknet.ninjal.ac.jp/nuc/templates/nuc.html)

## seq2seqのコード

seq2seqをそのまま試せるコードをアップしていただいているので、cloneして動かしてみます。あ、ちなみにPython2系でしか動かないようなのでご注意を。

```
$ git clone https://github.com/knok/tf-seq2seq-mod
$ cd tf-seq2seq-mod
```

学習時の入力ファイルと出力ファイルの保存先のディレクトリを作ります。

```
$ mkdir train
$ mkdir data
```

先程、準備編で作成した`input.txt`と`output.txt`を`data`ディレクトリに移動しておきます。

```
$ mv ../make-meidai-dialogue/input.txt data
$ mv ../make-meidai-dialogue/output.txt data
```

これで事前準備はOK。seq2seqの学習処理を走らせます。パラメータはとりあえず元記事のままで。

```
$ python translate.py --data_dir data --train_dir train \
--size 400 \
--en_vocab_size 10000 \
--fr_vocab_size 10000 \
--num_layers 1 \
--batch_size 5

Creating 1 layers of 400 units.
WARNING:tensorflow:From /Users/hogehoge/source/tf-seq2seq-mod/seq2seq_model.py:195 in __init__.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.
Created model with fresh parameters.
WARNING:tensorflow:From translate.py:148 in create_model.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Reading training data (limit: 0).
global step 200 learning rate 0.5000 step-time 0.79 perplexity 1109.75
  trg = そっ か そっ か そっ か 。
  hyp = うん 、 、
global step 400 learning rate 0.5000 step-time 0.77 perplexity 79.50
  trg = _UNK もの 。
  hyp = うん 、 、
global step 600 learning rate 0.5000 step-time 0.82 perplexity 61.00
  trg = そう です よ ね 。
  hyp = そう 。 。 。 。
global step 800 learning rate 0.5000 step-time 1.68 perplexity 46.11
  trg = ドジョウ すくい とか 。
  hyp = うん 、 、 て
global step 1000 learning rate 0.5000 step-time 1.22 perplexity 47.88
  trg = そう か ね 。
  hyp = うん 。 。 。

略
```

なんと、自宅PCにはGPUを積んでいなかったので仕方なくCPUだけで走らせましたが、CPUだけでディープラーニングは絶望的ですねー。とりあえず丸1日くらい放置してみました。

```
global step 33000 learning rate 0.3699 step-time 1.29 perplexity 10.53
  trg = え 、 いい や 、 これ で 。
  hyp = うん ? うん 。 。 いい 。 。
global step 33200 learning rate 0.3699 step-time 1.47 perplexity 10.08
  trg = うん 。
  hyp = うん 。
global step 33400 learning rate 0.3699 step-time 1.47 perplexity 10.19
  trg = A 0000 ST 、 by サンヨー です 。
  hyp = うん の 円 ? それ 、 だ よ
global step 33600 learning rate 0.3699 step-time 1.56 perplexity 9.85
  trg = 行く 方 が ね 。
  hyp = あっ ん が 。 。
```

`perplexity`が収束度合いの指標になるのですが、`1`あたりまで到達するみたい。CPUだけではこのあたりが限界でしたので、このへんで強制停止しました。

元記事を見ると`1`あたりに収束するまでに467,000ステップと。。まじかよこの10倍以上じゃないか。CPUもうむりぽ。。

## 対話させてみる

`--decode`オプションをつけると対話モードになるみたい。形態素解析後の分かち書きされた文章のinputが必要なのでそんな感じで対話してみよう。

```
$ python translate.py --data_dir data --train_dir train \
 --size 400 --en_vocab_size 10000 --fr_vocab_size 10000 --decode
```

```
WARNING:tensorflow:From /Users/hogehoge/source/tf-seq2seq-mod/seq2seq_model.py:195 in __init__.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.
Created model with fresh parameters.
WARNING:tensorflow:From translate.py:148 in create_model.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.

> こんにちは
転々 転々 転々 転々 転々 転々 転々 転々 転々 転々
転々 転々 転々 転々 転々 転々 転々 転々 転々 転々
>  なん すか これ
いて 転々 転々 転々 転々 転々 転々 転々 転々 転々
> 元気 です か ？
劣化 劣化 転々 転々 転々 転々 転々 転々 いて いて
```

文字通りお話になりませんでした。うーん、他の方が同様のことをやってる記事みたけど、もっと少ないステップ数でもそれっぽく動いてるのあったんだけどなー。

トライアンドエラーをするのに時間がかかりすぎてつらいので、次はGPU使って学習させてみようかな。

## おまけ

エラーログの記録をとっていなかったから書いてませんが`pip install numpy`とか`pip install tensorflow`とかやった気がする。

